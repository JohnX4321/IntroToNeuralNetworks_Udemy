{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = torchvision.models.resnet101()\n",
    "num_iterations = 10\n",
    "xe = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)"
   ],
   "id": "6a4c4956e6d5d973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "batch_size = 50   # this works, 100 does not\n",
    "for i in range(num_iterations):\n",
    "    inputs = torch.randn(batch_size,3,224,224)\n",
    "    labels = torch.LongTensor(batch_size).random_(0, 100)\n",
    "    loss = xe(model(inputs), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Done one batch\")"
   ],
   "id": "e13cc961a9546b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "desired_batch_size = 100\n",
    "tolerable_batch_size = 50\n",
    "accum_steps = desired_batch_size / tolerable_batch_size"
   ],
   "id": "9ebfbd410c5ec04a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(num_iterations):\n",
    "    inputs = torch.randn(tolerable_batch_size,3,224,224)\n",
    "    labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n",
    "    loss = xe(model(inputs), labels)\n",
    "    # The loss needs to be scaled to have the same significance over the whole dataset, because the mean should be taken across\n",
    "    # the whole dataset , which requires the loss to be divided by the number of batches. This is only the case if the loss\n",
    "    # returned is averaged. But if the loss returned is summed (for example: nn.BCELoss(reduction = 'sum')) then we do not need to do this\n",
    "    loss = loss / accum_steps\n",
    "    loss.backward()\n",
    "\n",
    "    if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Done one batch\")"
   ],
   "id": "9513276eb2b79a32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
