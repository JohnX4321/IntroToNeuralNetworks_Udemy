{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx={}\n",
    "        self.idx2word={}\n",
    "        self.idx=0\n",
    "\n",
    "    def add_word(self,word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ],
   "id": "2119c9069e059fd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TextProcess(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self,path,batch_size=20):\n",
    "        with open(path,\"r\") as f:\n",
    "            tokens =0\n",
    "            for line in f:\n",
    "                words=line.split()+['<eos>']\n",
    "                tokens+=len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        #Create 1d tensor index of all words\n",
    "        rep_tensor=torch.LongTensor(tokens)\n",
    "        index=0\n",
    "        with open(path,'r') as f:\n",
    "            for line in f:\n",
    "                words=line.split()+['<eos>']\n",
    "                for word in words:\n",
    "                    rep_tensor[index]=self.dictionary.word2idx[word]\n",
    "                    index+=1\n",
    "         #Find out how many batches we need\n",
    "        num_batches = rep_tensor.shape[0] // batch_size\n",
    "        #Remove the remainder (Filter out the ones that don't fit)\n",
    "        rep_tensor = rep_tensor[:num_batches*batch_size]\n",
    "        # return (batch_size,num_batches)\n",
    "        rep_tensor = rep_tensor.view(batch_size, -1)\n",
    "        return rep_tensor"
   ],
   "id": "4d98bb39ca1d6131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embed_size = 128    #Input features to the LSTM\n",
    "hidden_size = 1024  #Number of LSTM units\n",
    "num_layers = 1\n",
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "timesteps = 30\n",
    "learning_rate = 0.002"
   ],
   "id": "f4fedc4cc2030ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "corpus = TextProcess()",
   "id": "7162fbebfd7cfb65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "rep_tensor = corpus.get_data('alice.txt', batch_size)",
   "id": "eafcb1edc06951f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#rep_tensor is the tensor that contains the index of all the words. Each row contains 1659 words by default\n",
    "print(rep_tensor.shape)"
   ],
   "id": "cc2598cd487049a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ],
   "id": "ffbfa934c66fe863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_batches = rep_tensor.shape[1] // timesteps\n",
    "print(num_batches)"
   ],
   "id": "5a139645e9f655ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TextGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Perform Word Embedding\n",
    "        x = self.embed(x)\n",
    "        #Reshape the input tensor\n",
    "        #x = x.view(batch_size,timesteps,embed_size)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        # Reshape the output from (samples,timesteps,output_features) to a shape appropriate for the FC layer\n",
    "        # (batch_size*timesteps, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ],
   "id": "3abee017228b3562"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)"
   ],
   "id": "618ad264db95c077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def detach(states):\n",
    "    \"\"\"\n",
    "If we have a tensor z,'z.detach()' returns a tensor that shares the same storage\n",
    "as 'z', but with the computation history forgotten. It doesn't know anything\n",
    "about how it was computed. In other words, we have broken the tensor z away from its past history\n",
    "Here, we want to perform truncated Backpropagation\n",
    "TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as\n",
    "a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal\n",
    "dependencies that span more than 20 timesteps.\n",
    "    \"\"\"\n",
    "    return [state.detach() for state in states]"
   ],
   "id": "2c2f75113edb5091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "bafd8608d321719d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size))\n",
    "\n",
    "    for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = rep_tensor[:, i:i+timesteps]\n",
    "        targets = rep_tensor[:, (i+1):(i+1)+timesteps]\n",
    "\n",
    "        outputs,_ = model(inputs, states)\n",
    "        loss = loss_fn(outputs, targets.reshape(-1))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        #Perform Gradient Clipping. clip_value (float or int) is the maximum allowed value of the gradients\n",
    "        #The gradients are clipped in the range [-clip_value, clip_value]. This is to prevent the exploding gradient problem\n",
    "        clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // timesteps\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, loss.item()))"
   ],
   "id": "1e8d7fdfeb6c66a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    with open('results.txt', 'w') as f:\n",
    "        # Set intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size),\n",
    "                 torch.zeros(num_layers, 1, hidden_size))\n",
    "        # Select one word id randomly and convert it to shape (1,1)\n",
    "        input = torch.randint(0,vocab_size, (1,)).long().unsqueeze(1)\n",
    "\n",
    "        for i in range(500):\n",
    "            output, _ = model(input, state)\n",
    "            print(output.shape)\n",
    "            # Sample a word id from the exponential of the output\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "            print(word_id)\n",
    "            # Replace the input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # Write the results to file\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))"
   ],
   "id": "3d7877f284c95dce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
